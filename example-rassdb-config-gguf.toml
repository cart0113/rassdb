# Example configuration for using nomic-embed-code GGUF model
# Copy this to ~/.rassdb-config.toml to use the GGUF model by default

[embedding-model]
# Use the GGUF quantized version of nomic-embed-code
# This is a 7B parameter model based on Qwen2.5 architecture
# 8-bit quantized version (~7GB) with 3584-dimensional embeddings
name = "nomic-ai/nomic-embed-code-gguf"

# Alternative: Use the shorter alias
# name = "nomic-embed-code-gguf"

# Note: This model requires llama-cpp-python to be installed:
# pip install llama-cpp-python

# The model file should be located at:
# ~/.cache/huggingface/hub/models--nomic-ai--nomic-embed-code/blobs/nomic-embed-code.Q8_0.gguf

# IMPORTANT: This GGUF model has 3584-dimensional embeddings (not 768)
# It's a different, more powerful architecture than the standard nomic-embed-code

# Other available models:
# name = "nomic-ai/nomic-embed-text-v1.5"  # Default text embedding model (768 dims)
# name = "nomic-ai/nomic-embed-code"       # Standard code model (768 dims, requires ~1.4GB)
# name = "nomic-cloud/nomic-embed-code"    # Cloud API version (requires NOMIC_API_KEY)

# Include/exclude patterns can be added here as well
# See the main documentation for more configuration options